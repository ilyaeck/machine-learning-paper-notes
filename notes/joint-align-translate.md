# Neural Machine Translation By Jointly Learning to Align and Translate

### The pitch: prior encoder-decoder (seq2seq) approaches translation have a major bottleneck: input sequences must be
squashed into a fixed-length vector. This translations to break down for long sentences. Here, *attention* comes to resque
and removes the fixed-length constraint.
si =f(s_{i−1},y_{i−1},c_i). 

*
